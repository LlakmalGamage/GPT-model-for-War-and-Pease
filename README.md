# GPT Model for "War and Peace"

This repository contains the code for training a GPT model on the full text of Leo Tolstoy's "War and Peace." The model is implemented using PyTorch and is designed to generate text in the style of the novel.

## Project Overview

The goal of this project is to build a language model that can generate coherent and stylistically consistent text based on "War and Peace." The model is trained on the entire text of the novel and uses a transformer architecture inspired by the GPT (Generative Pretrained Transformer) models.

### Model Architecture

- **Embedding Dimension (`n_embd`)**: 300
- **Number of Attention Heads (`n_head`)**: 11
- **Number of Transformer Layers (`n_layer`)**: 10
- **Dropout**: 0.2
- **Context Length (`block_size`)**: 256 tokens

The model is trained using the AdamW optimizer with a learning rate of 3e-4.

### Dataset

The training data is derived from the full text of "War and Peace." The text is tokenized at the character level, and the vocabulary consists of all unique characters in the text.

### Training

- **Batch Size**: 64
- **Maximum Iterations (`max_iters`)**: 4500
- **Evaluation Interval**: Every 500 iterations

### Loss Estimation

The model's performance is evaluated on both the training and validation sets every 500 iterations. The training and validation losses are calculated using cross-entropy.

### Text Generation

The model can generate text by sampling from the probability distribution of the next character given a context. The `generate` function allows the generation of sequences with a specified maximum number of tokens.

### Example Output

Here is an example of text generated by the model: 
' interrupt for this day before attached, or Obod.... and the enemy was!
Take nonsense, the Russian army, to the friends of mailholiers smalling from
ther object for the Emperor and to the Emperor Alexander of materiates other to Talka.
His heart had been at wiciding head, plushed similarly, now hasted a
sette of him,
rewarded with general lady, in which he had to confess
when the imprivently prepared fact so rather on. He did not enter
apply any range down. Look dinner into the enommkhaber impro ' 

